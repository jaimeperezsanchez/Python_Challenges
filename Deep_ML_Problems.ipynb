{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 6 - Calculate Eigenvalues of a Matrix\n",
        "Write a Python function that calculates the eigenvalues of a 2x2 matrix. The function should return a list containing the eigenvalues, sort values from highest to lowest."
      ],
      "metadata": {
        "id": "uhx9vliNCkgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXMdSiPcCgfE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n",
        "\teigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
        "\treturn list(eigenvalues)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_eigenvalues([[4, -2], [1, 1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2TjPTwPC00Z",
        "outputId": "d74346e5-253b-4eab-fad9-64894f55b32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.0, 2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_eigenvalues_manual(matrix: list[list[float|int]]) -> list[float]:\n",
        "  a, b = matrix[0]\n",
        "  c, d = matrix[1]\n",
        "\n",
        "  trace = a + d  # Sum of diagonal elements\n",
        "  determinant = a * d - b * c  # Determinant of the matrix\n",
        "  discriminant = (trace ** 2) - (4 * determinant)  # Compute the discriminant\n",
        "\n",
        "  sqrt_discriminant = discriminant ** 0.5  # Compute square root\n",
        "\n",
        "  # Compute eigenvalues\n",
        "  lambda1 = (trace + sqrt_discriminant) / 2\n",
        "  lambda2 = (trace - sqrt_discriminant) / 2\n",
        "\n",
        "  return [lambda1, lambda2]  # Sort in descending order"
      ],
      "metadata": {
        "id": "6EcSLpiDEZep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_eigenvalues_manual([[4, -2], [1, 1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KtBAffiElyx",
        "outputId": "bfc17873-8043-4607-f4dc-1e2bccefae2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.0, 2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 7 - Matrix Transformation\n",
        "Write a Python function that transforms a given matrix A using the operation $T^{-1} AS$, where T and S are invertible matrices. The function should first validate if the matrices T and S are invertible, and then perform the transformation. In cases where there is no solution return -1"
      ],
      "metadata": {
        "id": "xaWBY2IMapWL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17A24p6_apWL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Union\n",
        "\n",
        "def transform_matrix(\n",
        "    A: list[list[Union[int, float]]],\n",
        "    T: list[list[Union[int, float]]],\n",
        "    S: list[list[Union[int, float]]]\n",
        ") -> list[list[Union[int, float]]]:\n",
        "    \"\"\"\n",
        "    Transforms the matrix A using the operation T^{-1} A S.\n",
        "\n",
        "    Parameters:\n",
        "        A: A matrix represented as a list of lists, where each element is int or float.\n",
        "        T: An invertible square matrix. The number of rows of A must equal the size of T.\n",
        "        S: An invertible square matrix. The number of columns of A must equal the size of S.\n",
        "\n",
        "    Returns:\n",
        "        The transformed matrix as a list of lists if T and S are invertible and dimensions match,\n",
        "        otherwise returns -1.\n",
        "    \"\"\"\n",
        "    # Validate that T is a non-empty square matrix.\n",
        "    if not T or any(len(row) != len(T) for row in T):\n",
        "        return -1\n",
        "\n",
        "    # Validate that S is a non-empty square matrix.\n",
        "    if not S or any(len(row) != len(S) for row in S):\n",
        "        return -1\n",
        "\n",
        "    # Validate that A has dimensions compatible with T and S.\n",
        "    if len(A) != len(T) or any(len(row) != len(S) for row in A):\n",
        "        return -1\n",
        "\n",
        "    # Convert lists to NumPy arrays for computation.\n",
        "    T_arr = np.array(T, dtype=float)\n",
        "    A_arr = np.array(A, dtype=float)\n",
        "    S_arr = np.array(S, dtype=float)\n",
        "\n",
        "    # Validate that T is invertible.\n",
        "    try:\n",
        "        T_inv = np.linalg.inv(T_arr)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return -1\n",
        "\n",
        "    # Validate that S is invertible (even though we don't use S^{-1} in the transformation).\n",
        "    try:\n",
        "        _ = np.linalg.inv(S_arr)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return -1\n",
        "\n",
        "    # Compute the transformation: T^{-1} * A * S.\n",
        "    result = T_inv @ A_arr @ S_arr\n",
        "\n",
        "    # Convert the result back to a list of lists.\n",
        "    return result.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = [[1, 2], [3, 4]]\n",
        "T = [[2, 0], [0, 2]]\n",
        "S = [[1, 1], [0, 1]]\n",
        "print(transform_matrix(A, T, S))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebcz3VzweucF",
        "outputId": "a69abce1-f3f4-4eff-cc77-4388b650b35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5, 1.5], [1.5, 3.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 8 - Calculate 2x2 Matrix Inverse\n",
        "Write a Python function that calculates the inverse of a 2x2 matrix. Return 'None' if the matrix is not invertible."
      ],
      "metadata": {
        "id": "mM8LlfOGifhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8C2sC7AifhT"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def inverse_2x2(matrix: List[List[float]]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Computes the inverse of a 2x2 matrix.\n",
        "\n",
        "    Parameters:\n",
        "        matrix: A 2x2 matrix represented as a list of lists of floats.\n",
        "\n",
        "    Returns:\n",
        "        The inverse of the input matrix as a list of lists if the matrix is invertible,\n",
        "        otherwise returns -1.\n",
        "    \"\"\"\n",
        "    # Check that the input matrix is non-empty and square (i.e., has the same number of rows and columns).\n",
        "    if not matrix or any(len(row) != len(matrix) for row in matrix):\n",
        "        return -1\n",
        "\n",
        "    # Convert the input list-of-lists into a NumPy array for numerical computation.\n",
        "    M_arr = np.array(matrix, dtype=float)\n",
        "\n",
        "    # Attempt to compute the inverse of the matrix using NumPy's linear algebra module.\n",
        "    # If the matrix is singular (non-invertible), a LinAlgError is raised, and we return -1.\n",
        "    try:\n",
        "        M_inv = np.linalg.inv(M_arr)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return -1\n",
        "\n",
        "    # Convert the resulting NumPy array back to a list-of-lists and return it.\n",
        "    return M_inv.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = [[4, 7], [2, 6]]\n",
        "print(inverse_2x2(matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26ba9f4-853d-4669-d341-e223cebdc7f9",
        "id": "-n1JrJSgifhT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6000000000000001, -0.7000000000000001], [-0.2, 0.4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 9 - Matrix times Matrix\n",
        "Multiply two matrices together (return -1 if shapes of matrix dont aline), i.e. C=A⋅B"
      ],
      "metadata": {
        "id": "OBE6leHdk5aL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPD8wRjLk5aL"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def matrixmul(A: list[list[int | float]],\n",
        "              B: list[list[int | float]]) -> list[list[int | float]] | int:\n",
        "    # Get the dimensions of the matrices\n",
        "    rows_a, cols_a = len(A), len(A[0]) if A else 0\n",
        "    rows_b, cols_b = len(B), len(B[0]) if B else 0\n",
        "\n",
        "    # Check if multiplication is possible\n",
        "    if cols_a != rows_b:\n",
        "        return -1  # Matrices are not aligned\n",
        "\n",
        "    A_arr = np.array(A, dtype=float)\n",
        "    B_arr = np.array(B, dtype=float)\n",
        "\n",
        "    # Perform matrix multiplication\n",
        "    result = A_arr @ B_arr\n",
        "\n",
        "    return result.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrixmul(A = [[1,2],[2,4]], B = [[2,1],[3,4]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d2f91a-3517-4cfc-c8c0-8b395405010a",
        "id": "HLNwkjntk5aL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.0, 9.0], [16.0, 18.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 11 - Solve Linear Equations using Jacobi Method\n",
        "Write a Python function that uses the Jacobi method to solve a system of linear equations given by Ax = b. The function should iterate n times, rounding each intermediate solution to four decimal places, and return the approximate solution x."
      ],
      "metadata": {
        "id": "ei6ooTOsoJHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ihUyq7DoJHU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def solve_jacobi(A, b, iterations):\n",
        "    A = np.array(A, dtype=float)\n",
        "    b = np.array(b, dtype=float)\n",
        "    n = A.shape[0]\n",
        "    x = np.zeros(n)\n",
        "\n",
        "    # Precompute the inverse of the diagonal elements of A\n",
        "    D_inv = 1 / np.diag(A)\n",
        "    # Compute the off-diagonal part of A\n",
        "    R = A - np.diag(np.diag(A))\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Compute the new x using the Jacobi update formula\n",
        "        x = D_inv * (b - np.dot(R, x))\n",
        "        # Round the solution to 4 decimal places after each iteration\n",
        "        x = np.round(x, 4)\n",
        "\n",
        "    return x.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def solve_jacobi_(A: np.ndarray, b: np.ndarray, n: int) -> list:\n",
        "    d_a = np.diag(A)\n",
        "    nda = A - np.diag(d_a)\n",
        "    x = np.zeros(len(b))\n",
        "    x_hold = np.zeros(len(b))\n",
        "    for _ in range(n):\n",
        "        for i in range(len(A)):\n",
        "            x_hold[i] = (1/d_a[i]) * (b[i] - sum(nda[i]*x))\n",
        "        x = x_hold.copy()\n",
        "    return np.round(x,4).tolist()"
      ],
      "metadata": {
        "id": "pgj48Xw0rp65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1276990c-17c7-4633-e0cf-8084cbad5a89",
        "id": "rV6EdCjToJHU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.146, 0.2032, -0.5175]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 17 - K-Means Clustering\n",
        "Your task is to write a Python function that implements the k-Means clustering algorithm. This function should take specific inputs and produce a list of final centroids. k-Means clustering is a method used to partition n points into k clusters. The goal is to group similar points together and represent each group by its center (called the centroid).\n",
        "Function Inputs:\n",
        "  * points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n",
        "  * k: An integer representing the number of clusters to form\n",
        "  * initial_centroids: A list of initial centroid points, each a tuple of coordinates\n",
        "  * max_iterations: An integer representing the maximum number of iterations to perform\n",
        "\n",
        "Function Output:\n",
        "\n",
        "A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal."
      ],
      "metadata": {
        "id": "3QyGXCCJsNWm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjmnX9_AsNWn"
      },
      "outputs": [],
      "source": [
        "def k_means_clustering(points, k, initial_centroids, max_iterations):\n",
        "    centroids = [tuple(centroid) for centroid in initial_centroids]\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        # Assign each point to the nearest centroid\n",
        "        clusters = [[] for _ in range(k)]\n",
        "        for point in points:\n",
        "            # Calculate squared distances to avoid sqrt for efficiency\n",
        "            distances = [sum((p - c) ** 2 for p, c in zip(point, centroid)) for centroid in centroids]\n",
        "            closest = distances.index(min(distances))\n",
        "            clusters[closest].append(point)\n",
        "\n",
        "        # Compute new centroids\n",
        "        new_centroids = []\n",
        "        for i in range(k):\n",
        "            cluster_points = clusters[i]\n",
        "            if cluster_points:\n",
        "                # Calculate the mean of each coordinate\n",
        "                sum_coords = [sum(dim) for dim in zip(*cluster_points)]\n",
        "                avg = tuple(coord_sum / len(cluster_points) for coord_sum in sum_coords)\n",
        "            else:\n",
        "                # If no points, keep the previous centroid\n",
        "                avg = centroids[i]\n",
        "            new_centroids.append(avg)\n",
        "\n",
        "        # Check for convergence\n",
        "        if new_centroids == centroids:\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    # Round each coordinate of the centroids to the nearest fourth decimal\n",
        "    rounded_centroids = [tuple(round(coord, 4) for coord in centroid) for centroid in centroids]\n",
        "    return rounded_centroids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10))\n",
        "# Expected [(1.0, 2.0), (10.0, 2.0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a497c8fe-9c2e-43c7-d39e-594f289c9fe8",
        "id": "ybtPPcc6sNWo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1.0, 2.0), (10.0, 2.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 18 - Cross-Validation Data Split Implementation\n",
        "Write a Python function that performs k-fold cross-validation data splitting from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature) and an integer k representing the number of folds. The function should split the dataset into k parts, systematically use one part as the test set and the remaining as the training set, and return a list where each element is a tuple containing the training set and test set for each fold."
      ],
      "metadata": {
        "id": "lsuaFchIJhaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcwZQmECJhaV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_validation_split(data: np.ndarray, k: int, seed=42) -> list:\n",
        "    np.random.seed(seed)\n",
        "    n_samples = data.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    folds = np.array_split(indices, k)\n",
        "    k_folds = []\n",
        "\n",
        "    for i in range(k):\n",
        "        test_indices = folds[i]\n",
        "        train_indices = np.concatenate([folds[j] for j in range(k) if j != i])\n",
        "        train_set = data[train_indices].tolist()\n",
        "        test_set = data[test_indices].tolist()\n",
        "        k_folds.append((train_set, test_set))\n",
        "\n",
        "    return k_folds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_validation_split(data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), k = 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f92f9a3-5392-46e2-a8f3-5c19bbc874e3",
        "id": "FJloPg7IJhaW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[([[9, 10], [5, 6], [1, 2], [7, 8]], [[3, 4]]), ([[3, 4], [5, 6], [1, 2], [7, 8]], [[9, 10]]), ([[3, 4], [9, 10], [1, 2], [7, 8]], [[5, 6]]), ([[3, 4], [9, 10], [5, 6], [7, 8]], [[1, 2]]), ([[3, 4], [9, 10], [5, 6], [1, 2]], [[7, 8]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 19 - Principal Component Analysis (PCA) Implementation\n",
        "Write a Python function that performs Principal Component Analysis (PCA) from scratch. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function should standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components (the eigenvectors corresponding to the largest eigenvalues). The function should also take an integer k as input, representing the number of principal components to return.\n",
        "\n",
        "Reasoning:\n",
        "After standardizing the data and computing the covariance matrix, the eigenvalues and eigenvectors are calculated. The largest eigenvalue's corresponding eigenvector is returned as the principal component, rounded to four decimal places."
      ],
      "metadata": {
        "id": "A6SIF_r6OQCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqbmHHH7OQCK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pca(data: np.ndarray, k: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs Principal Component Analysis (PCA) from scratch.\n",
        "    Args:\n",
        "        X (numpy.ndarray): A 2D NumPy array where each row is a data sample\n",
        "                           and each column represents a feature.\n",
        "        k (int): The number of principal components to return.\n",
        "    Returns:\n",
        "        numpy.ndarray: A NumPy array containing the top k principal components\n",
        "                       (eigenvectors corresponding to the largest eigenvalues).\n",
        "                       Each row is a principal component.\n",
        "    \"\"\"\n",
        "    # 1. Standardize the dataset\n",
        "    mean = np.mean(data, axis=0)\n",
        "    std_dev = np.std(data, axis=0)\n",
        "    data_standardized = (data - mean) / std_dev\n",
        "\n",
        "    # 2. Compute the covariance matrix\n",
        "    covariance_matrix = np.cov(data_standardized.T) # Transpose X_standardized to get features as rows\n",
        "\n",
        "    # 3. Compute eigenvalues and eigenvectors\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "    # 4. Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
        "    eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
        "    eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # 5. Select top k eigenvectors (principal components)\n",
        "    principal_components = np.array([pair[1] for pair in eigen_pairs[:k]]).T # Transpose to have PC as rows\n",
        "\n",
        "    return np.round(principal_components, 4).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca(data = np.array([[1, 2], [3, 4], [5, 6]]), k = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478c6123-1411-4ab1-8cf2-e95e67f6149d",
        "id": "KeupVuy-OQCL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7071], [0.7071]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 25 - Single Neuron with Backpropagation\n",
        "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places."
      ],
      "metadata": {
        "id": "qIqps27ySM72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP--ddAHSM72"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n",
        "    weights = np.array(initial_weights, dtype=np.float64)\n",
        "    bias = np.float64(initial_bias)\n",
        "    mse_values = []\n",
        "    num_samples = len(features)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        grad_weights = np.zeros_like(weights)\n",
        "        grad_bias = 0.0\n",
        "\n",
        "        for x, y in zip(features, labels):\n",
        "            x_array = np.array(x, dtype=np.float64)\n",
        "            z = np.dot(weights, x_array) + bias\n",
        "            a = sigmoid(z)\n",
        "            loss = (a - y) ** 2\n",
        "            total_loss += loss\n",
        "\n",
        "            # Compute gradients\n",
        "            da = 2 * (a - y)\n",
        "            dz = da * a * (1 - a)\n",
        "            dw = dz * x_array\n",
        "            db = dz\n",
        "\n",
        "            grad_weights += dw\n",
        "            grad_bias += db\n",
        "\n",
        "        # Average the gradients and loss\n",
        "        avg_loss = total_loss / num_samples\n",
        "        avg_grad_weights = grad_weights / num_samples\n",
        "        avg_grad_bias = grad_bias / num_samples\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * avg_grad_weights\n",
        "        bias -= learning_rate * avg_grad_bias\n",
        "\n",
        "        # Record MSE for this epoch\n",
        "        mse_values.append(round(avg_loss, 4))\n",
        "\n",
        "    # Round the final parameters to four decimal places\n",
        "    updated_weights = [round(w, 4) for w in weights]\n",
        "    updated_bias = round(bias, 4)\n",
        "\n",
        "    return updated_weights, updated_bias, mse_values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_neuron(features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2))"
      ],
      "metadata": {
        "id": "ZPr0yGwgVzOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 26 - Implementing Basic Autograd Operations\n",
        "Special thanks to Andrej Karpathy for making a video about this, if you haven't already check out his videos on YouTube https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg. Write a Python class similar to the provided 'Value' class that implements the basic autograd operations: addition, multiplication, and ReLU activation. The class should handle scalar values and should correctly compute gradients for these operations through automatic differentiation."
      ],
      "metadata": {
        "id": "cMmCHOqKVsC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imG-of4EVsC8"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0  # Initialize grad as an integer\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += int(round(out.grad))  # Round and convert to integer\n",
        "            other.grad += int(round(out.grad))  # Round and convert to integer\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        self_data = self.data\n",
        "        other_data = other.data\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += int(round(out.grad * other_data))  # Round and convert to integer\n",
        "            other.grad += int(round(out.grad * self_data))  # Round and convert to integer\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return self + other\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return self * other\n",
        "\n",
        "    def relu(self):\n",
        "        out_data = self.data if self.data > 0 else 0.0\n",
        "        out = Value(out_data, (self,), 'ReLU')\n",
        "\n",
        "        derivative = 1.0 if self.data > 0 else 0.0\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += int(round(out.grad * derivative))  # Round and convert to integer\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        topo.reverse()\n",
        "\n",
        "        self.grad = 1  # Initialize output gradient as an integer\n",
        "        for node in topo:\n",
        "            node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2)\n",
        "b = Value(-3)\n",
        "c = Value(10)\n",
        "d = a + b * c\n",
        "e = d.relu()\n",
        "e.backward()\n",
        "print(a, b, c, d, e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04901140-1b95-4410-8f49-393bbe5ce861",
        "id": "CqipLQaEVsC9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0) Value(data=-28, grad=0) Value(data=0.0, grad=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 31 - Divide Dataset Based on Feature Threshold\n",
        "Write a Python function to divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold. The function should return two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n"
      ],
      "metadata": {
        "id": "ifmf1JCJrC0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST-QltIYrC0i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def divide_on_feature(X, feature_i, threshold):\n",
        "    # Create a boolean mask for samples where the feature value is >= threshold\n",
        "    mask = X[:, feature_i] >= threshold\n",
        "    # Subset the dataset based on the mask\n",
        "    meet_condition = X[mask]\n",
        "    not_meet_condition = X[~mask]\n",
        "    return meet_condition, not_meet_condition\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2],\n",
        "              [3, 4],\n",
        "              [5, 6],\n",
        "              [7, 8],\n",
        "              [9, 10]])\n",
        "feature_i = 0\n",
        "threshold = 5\n",
        "\n",
        "print(divide_on_feature(X, feature_i, threshold))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9acd0e-6547-4da3-bbab-787696b43bc7",
        "id": "osnMh_LTrC0j"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[ 5,  6],\n",
            "       [ 7,  8],\n",
            "       [ 9, 10]]), array([[1, 2],\n",
            "       [3, 4]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 32 - Generate Polynomial Features\n",
        "Write a Python function to generate polynomial features for a given dataset. The function should take in a 2D numpy array X and an integer degree, and return a new 2D numpy array with polynomial features up to the specified degree.\n"
      ],
      "metadata": {
        "id": "5A5uMXBcsMlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1kgiMnvsMlR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import combinations_with_replacement\n",
        "\n",
        "def polynomial_features(X, degree):\n",
        "    n_samples, n_features = X.shape\n",
        "    features = []\n",
        "    # Iterate over degrees 0 to degree (degree 0 corresponds to the constant feature)\n",
        "    for d in range(degree + 1):\n",
        "        # Generate all combinations of feature indices with replacement\n",
        "        for comb in combinations_with_replacement(range(n_features), d):\n",
        "            if len(comb) == 0:\n",
        "                # For degree 0, add the constant feature (ones)\n",
        "                feature = np.ones((n_samples, 1))\n",
        "            else:\n",
        "                # Multiply the selected features for the current combination\n",
        "                feature = np.prod(X[:, comb], axis=1, keepdims=True)\n",
        "            features.append(feature)\n",
        "    return np.concatenate(features, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[2, 3],\n",
        "                  [3, 4],\n",
        "                  [5, 6]])\n",
        "degree = 2\n",
        "print(polynomial_features(X, degree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0a4e67-bea5-4e71-a432-f88461e93b40",
        "id": "_UZYe29IsMlR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  2.  3.  4.  6.  9.]\n",
            " [ 1.  3.  4.  9. 12. 16.]\n",
            " [ 1.  5.  6. 25. 30. 36.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 33 - Generate Random Subsets of a Dataset\n",
        "Write a Python function to generate random subsets of a given dataset. The function should take in a 2D numpy array X, a 1D numpy array y, an integer n_subsets, and a boolean replacements. It should return a list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset). If replacements is True, the subsets should be created with replacements; otherwise, without replacements.\n",
        "\n"
      ],
      "metadata": {
        "id": "LMEX6wX2wU0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B_9rkUWwU0K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    n_samples = X.shape[0]\n",
        "    # Use full dataset size for bootstrap (with replacement)\n",
        "    # Use half the dataset size for subsampling (without replacement)\n",
        "    subset_size = n_samples if replacements else n_samples // 2\n",
        "    subsets = []\n",
        "    for _ in range(n_subsets):\n",
        "        indices = np.random.choice(n_samples, size=subset_size, replace=replacements)\n",
        "        X_subset = X[indices].tolist()\n",
        "        y_subset = y[indices].tolist()\n",
        "        subsets.append((X_subset, y_subset))\n",
        "    return subsets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    n, m = X.shape\n",
        "\n",
        "    subset_size = n if replacements else n // 2\n",
        "    idx = np.array([np.random.choice(n, subset_size, replace=replacements) for _ in range(n_subsets)])\n",
        "    # convert all ndarrays to lists\n",
        "    return [(X[idx][i].tolist(), y[idx][i].tolist()) for i in range(n_subsets)]\n"
      ],
      "metadata": {
        "id": "zQDKWd_PzDKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "print(get_random_subsets(X, y, 3, False, seed=42))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ef8b68-acab-4227-9af1-9b521c4098da",
        "id": "DueTLUdGwU0L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 37 - Calculate Correlation Matrix\n",
        "Write a Python function to calculate the correlation matrix for a given dataset. The function should take in a 2D numpy array X and an optional 2D numpy array Y. If Y is not provided, the function should calculate the correlation matrix of X with itself. It should return the correlation matrix as a 2D numpy array.\n",
        "\n"
      ],
      "metadata": {
        "id": "N7tnuvUpzNwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8qFaDtOzNwI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_correlation_matrix(X, Y=None):\n",
        "    if Y is None:\n",
        "        # Calculate the correlation matrix for X with itself.\n",
        "        # Set rowvar=False since each column is a variable.\n",
        "        return np.corrcoef(X, rowvar=False)\n",
        "    else:\n",
        "        # Ensure that X and Y have the same number of samples\n",
        "        if X.shape[0] != Y.shape[0]:\n",
        "            raise ValueError(\"X and Y must have the same number of rows (samples).\")\n",
        "\n",
        "        # Calculate means and center the data\n",
        "        X_mean = np.mean(X, axis=0)\n",
        "        Y_mean = np.mean(Y, axis=0)\n",
        "        X_centered = X - X_mean\n",
        "        Y_centered = Y - Y_mean\n",
        "\n",
        "        # Compute covariance matrix between columns of X and Y\n",
        "        cov_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n",
        "\n",
        "        # Compute standard deviations (using ddof=1 for sample std)\n",
        "        X_std = np.std(X, axis=0, ddof=1)\n",
        "        Y_std = np.std(Y, axis=0, ddof=1)\n",
        "\n",
        "        # Compute correlation matrix by dividing covariance by the outer product of stds\n",
        "        corr_matrix = cov_matrix / np.outer(X_std, Y_std)\n",
        "        return corr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"Correlation matrix for X with itself:\")\n",
        "print(calculate_correlation_matrix(X))\n",
        "\n",
        "# Correlation between X and Y\n",
        "Y = np.array([[2, 1], [4, 3], [6, 5]])\n",
        "print(\"\\nCorrelation matrix between X and Y:\")\n",
        "print(calculate_correlation_matrix(X, Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa29ae73-ad80-464b-deea-6a66abda883b",
        "id": "MK5b6gzrzNwI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation matrix for X with itself:\n",
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "\n",
            "Correlation matrix between X and Y:\n",
            "[[1. 1.]\n",
            " [1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 41 - Simple Convolutional 2D Layer\n",
        "In this problem, you need to implement a 2D convolutional layer in Python. This function will process an input matrix using a specified convolutional kernel, padding, and stride.\n"
      ],
      "metadata": {
        "id": "Nb0IQxMN0D-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vNv5JyQ0D-8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n",
        "    input_height, input_width = input_matrix.shape\n",
        "    kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "    # Pad the input matrix with zeros\n",
        "    padded_input = np.pad(input_matrix,\n",
        "                          pad_width=((padding, padding), (padding, padding)),\n",
        "                          mode='constant',\n",
        "                          constant_values=0)\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n",
        "    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n",
        "    output_matrix = np.zeros((output_height, output_width))\n",
        "\n",
        "    # Perform the convolution\n",
        "    for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "            h_start = i * stride\n",
        "            h_end = h_start + kernel_height\n",
        "            w_start = j * stride\n",
        "            w_end = w_start + kernel_width\n",
        "\n",
        "            # Extract the region of interest\n",
        "            region = padded_input[h_start:h_end, w_start:w_end]\n",
        "            output_matrix[i, j] = np.sum(region * kernel)\n",
        "\n",
        "    return output_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_matrix = np.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12],\n",
        "    [13, 14, 15, 16]\n",
        "])\n",
        "\n",
        "kernel = np.array([\n",
        "    [1, 0],\n",
        "    [-1, 1]\n",
        "])\n",
        "\n",
        "padding = 1\n",
        "stride = 2\n",
        "\n",
        "output = simple_conv2d(input_matrix, kernel, padding, stride)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63db2fad-772c-4270-9001-2b3024cf8204",
        "id": "sDbbiUTx0D-9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  1. -4.]\n",
            " [ 9.  7. -4.]\n",
            " [ 0. 14. 16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 47 - Implement Gradient Descent Variants with MSE Loss\n",
        "In this problem, you need to implement a single function that can perform three variants of gradient descent [Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent]  using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use."
      ],
      "metadata": {
        "id": "UrgGGK2Fi9tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_wTBgTWni9tm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
        "    m = len(y)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        if method == 'batch':\n",
        "            # Calculate the gradient using all data points\n",
        "            predictions = X.dot(weights)\n",
        "            errors = predictions - y\n",
        "            gradient = 2 * X.T.dot(errors) / m\n",
        "            weights = weights - learning_rate * gradient\n",
        "\n",
        "        elif method == 'stochastic':\n",
        "            # Update weights for each data point individually\n",
        "            for i in range(m):\n",
        "                prediction = X[i].dot(weights)\n",
        "                error = prediction - y[i]\n",
        "                gradient = 2 * X[i].T.dot(error)\n",
        "                weights = weights - learning_rate * gradient\n",
        "\n",
        "        elif method == 'mini_batch':\n",
        "            # Update weights using sequential batches of data points without shuffling\n",
        "            for i in range(0, m, batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                predictions = X_batch.dot(weights)\n",
        "                errors = predictions - y_batch\n",
        "                gradient = 2 * X_batch.T.dot(errors) / batch_size\n",
        "                weights = weights - learning_rate * gradient\n",
        "\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
        "y = np.array([2, 3, 4, 5])\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "n_iterations = 1000\n",
        "batch_size = 2\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.zeros(X.shape[1])\n",
        "\n",
        "# Test Batch Gradient Descent\n",
        "print(gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch'))\n",
        "# Test Stochastic Gradient Descent\n",
        "print(gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic'))\n",
        "# Test Mini-Batch Gradient Descent\n",
        "print(gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbedd990-3f30-4a70-a66a-f4d42e808b33",
        "id": "y_IgTfGui9tm"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.01003164 0.97050576]\n",
            "[1.00910568 0.9767164 ]\n",
            "[1.0100837  0.97053393]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 48 - Implement Reduced Row Echelon Form (RREF) Function\n",
        "In this problem, your task is to implement a function that converts a given matrix into its Reduced Row Echelon Form (RREF). The RREF of a matrix is a special form where each leading entry in a row is 1, and all other elements in the column containing the leading 1 are zeros, except for the leading 1 itself.\n",
        "\n",
        "However, there are some additional details to keep in mind:\n",
        "\n",
        "* Diagonal entries can be 0 if the matrix is reducible (i.e., the row corresponding to that position can be eliminated entirely).\n",
        "* Some rows may consist entirely of zeros.\n",
        "* If a column contains a pivot (a leading 1), all other entries in that column should be zero.\n",
        "\n",
        "Your task is to implement the RREF algorithm, which must handle these cases and convert any given matrix into its RREF."
      ],
      "metadata": {
        "id": "cR3Bo0BOnKqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VqKZDTnPnKqc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rref(matrix, tol=1e-10):\n",
        "    A = matrix.astype(np.float64).copy()  # work on a float copy\n",
        "    rows, cols = A.shape\n",
        "    pivot_row = 0\n",
        "\n",
        "    for col in range(cols):\n",
        "        # --- Partial pivoting ---\n",
        "        # Find the row with the largest absolute value in the current column (from pivot_row onward)\n",
        "        max_index = None\n",
        "        max_val = tol\n",
        "        for r in range(pivot_row, rows):\n",
        "            if abs(A[r, col]) > max_val:\n",
        "                max_val = abs(A[r, col])\n",
        "                max_index = r\n",
        "        if max_index is None:\n",
        "            continue  # no suitable pivot in this column\n",
        "\n",
        "        # Swap to put the best pivot row in position\n",
        "        if max_index != pivot_row:\n",
        "            A[[pivot_row, max_index]] = A[[max_index, pivot_row]]\n",
        "\n",
        "        # --- Normalize pivot row ---\n",
        "        pivot_val = A[pivot_row, col]\n",
        "        A[pivot_row, :] = A[pivot_row, :] / pivot_val\n",
        "\n",
        "        # --- Eliminate all other entries in this column ---\n",
        "        for r in range(rows):\n",
        "            if r != pivot_row:\n",
        "                factor = A[r, col]\n",
        "                A[r, :] = A[r, :] - factor * A[pivot_row, :]\n",
        "\n",
        "        pivot_row += 1\n",
        "        if pivot_row == rows:\n",
        "            break\n",
        "\n",
        "    # --- Post-processing: zero out tiny values ---\n",
        "    A[np.abs(A) < tol] = 0\n",
        "    return A\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix = np.array([\n",
        "    [1, 2, -1, -4],\n",
        "    [2, 3, -1, -11],\n",
        "    [-2, 0, -3, 22]\n",
        "])\n",
        "\n",
        "rref_matrix = rref(matrix)\n",
        "print(rref_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b7c0ce-1164-4ae0-aa37-d81966e96c0f",
        "id": "DECC9I9jnKqd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  0.  0. -8.]\n",
            " [ 0.  1.  0.  1.]\n",
            " [ 0.  0.  1. -2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 77 - Calculate Performance Metrics for a Classification Model\n",
        "In this task, you are required to implement a function performance_metrics(actual, predicted) that computes various performance metrics for a binary classification problem. These metrics include:\n",
        "\n",
        "* Confusion Matrix\n",
        "* Accuracy\n",
        "* F1 Score\n",
        "* Specificity\n",
        "* Negative Predictive Value\n",
        "\n",
        "The function should take in two lists:\n",
        "\n",
        "* actual: The actual class labels (1 for positive, 0 for negative).\n",
        "* predicted: The predicted class labels from the model.\n",
        "\n",
        "**Output**\n",
        "\n",
        "The function should return a tuple containing:\n",
        "\n",
        "* confusion_matrix: A 2x2 matrix.\n",
        "* accuracy: A float representing the accuracy of the model.\n",
        "* f1_score: A float representing the F1 score of the model.\n",
        "* specificity: A float representing the specificity of the model.\n",
        "* negative_predictive_value: A float representing the negative predictive value.\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "* All elements in the actual and predicted lists must be either 0 or 1.\n",
        "* Both lists must have the same length.\n"
      ],
      "metadata": {
        "id": "vvVAbp5mqlfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LdKn_qwCqlfO"
      },
      "outputs": [],
      "source": [
        "def performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n",
        "    if len(actual) != len(predicted):\n",
        "        raise ValueError(\"The actual and predicted lists must have the same length.\")\n",
        "\n",
        "    # Initialize confusion counts\n",
        "    TP = FN = FP = TN = 0\n",
        "    for a, p in zip(actual, predicted):\n",
        "        if a not in (0, 1) or p not in (0, 1):\n",
        "            raise ValueError(\"All elements in actual and predicted must be 0 or 1.\")\n",
        "        if a == 1 and p == 1:\n",
        "            TP += 1\n",
        "        elif a == 1 and p == 0:\n",
        "            FN += 1\n",
        "        elif a == 0 and p == 1:\n",
        "            FP += 1\n",
        "        elif a == 0 and p == 0:\n",
        "            TN += 1\n",
        "\n",
        "    confusion_matrix = [[TP, FN], [FP, TN]]\n",
        "    accuracy = (TP + TN) / len(actual)\n",
        "    f1 = (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) != 0 else 0.0\n",
        "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0.0\n",
        "    negativePredictive = TN / (TN + FN) if (TN + FN) != 0 else 0.0\n",
        "\n",
        "    return confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual = [1, 0, 1, 0, 1]\n",
        "predicted = [1, 0, 0, 1, 1]\n",
        "print(performance_metrics(actual, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ceb0f69-f850-44eb-9b66-a59625ff6e9b",
        "id": "Hxy1jnBDqlfP"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ID: 92 - Linear Regression - Power Grid Optimization\n",
        "\n",
        "It is the year 2157. Mars has its first thriving colony, and energy consumption is steadily on the rise. As the lead data scientist, you have daily power usage measurements (10 days) affected by both a growing linear trend and a daily fluctuation. The fluctuation follows the formula 10 x sin(2pi x i / 10), where i is the day number (1 through 10). Your challenge is to remove this known fluctuation from each data point, fit a linear regression model to the detrended data, predict day 15's base consumption, add back the fluctuation for day 15, and finally include a 5% safety margin. The final answer must be an integer, ensuring you meet the colony's future needs.\n",
        "\n",
        "**Reasoning:**\n",
        "\n",
        "For each of the 10 days, we subtract the daily fluctuation given by 10xsin(2πxi/10). We then perform linear regression on the resulting values, predict day 15’s base usage, and add back the day 15 fluctuation. Finally, we apply a 5% margin. Running the provided solution code yields 404 for this dataset."
      ],
      "metadata": {
        "id": "dWitFyOqzjMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_GY2eB7szjMP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "PI = 3.14159\n",
        "\n",
        "def power_grid_forecast(consumption_data):\n",
        "    # 1) Subtract the daily fluctuation from each data point.\n",
        "    detrended_consum = np.array([\n",
        "        daily_consum - (10 * math.sin(2 * PI * (i + 1) / 10))\n",
        "        for i, daily_consum in enumerate(consumption_data)\n",
        "    ]).reshape(-1, 1)\n",
        "\n",
        "    # 2) Prepare the design matrix for linear regression.\n",
        "    idxs = np.arange(1, len(consumption_data) + 1).reshape(-1, 1)\n",
        "    idxs_b = np.c_[np.ones((len(idxs), 1)), idxs]  # Design matrix with intercept term\n",
        "\n",
        "    # Correct the regression formula:\n",
        "    # theta = (X^T X)^{-1} X^T y, where X is idxs_b.\n",
        "    theta = np.linalg.inv(idxs_b.T @ idxs_b) @ idxs_b.T @ detrended_consum\n",
        "\n",
        "    # 3) Predict day 15's base consumption.\n",
        "    pred_15 = theta[0] + theta[1] * 15\n",
        "\n",
        "    # 4) Add the day 15 fluctuation back.\n",
        "    data_15_fluctuation = pred_15 + (10 * math.sin(2 * PI * 15 / 10))\n",
        "\n",
        "    # 5) Round, then add a 5% safety margin (rounded up).\n",
        "    data_15_margin = math.ceil(data_15_fluctuation[0] * 1.05)\n",
        "\n",
        "    # 6) Return the final integer.\n",
        "    return data_15_margin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "consumption_data = [150, 165, 185, 195, 210, 225, 240, 260, 275, 290]\n",
        "print(power_grid_forecast(consumption_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093d7d43-ff06-440a-91b1-fa6ebb3e1c58",
        "id": "nRp-hsFizjMQ"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404\n"
          ]
        }
      ]
    }
  ]
}